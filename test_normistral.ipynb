{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Example of using NorMistral on a GPU with low VRAM"
      ],
      "metadata": {
        "id": "YgKcuxQKprt9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize the environment"
      ],
      "metadata": {
        "id": "1Y07jLUIp6Tn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H48TM4h2Y-AR"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes\n",
        "!pip install accelerate\n",
        "\n",
        "import torch\n",
        "from google.colab import output\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "output.clear()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the model\n",
        "\n",
        "The model will be quantized into 8 bits, it trades off performance for less memory usage."
      ],
      "metadata": {
        "id": "qAEm5alCp2Q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'norallm/normistral-7b-warm' # @param [\"norallm/normistral-7b-warm\", \"norallm/normistral-7b-scratch\", \"norallm/norbloom-7b-scratch\"]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-C2u4Vb7pcGP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map='auto',\n",
        "    low_cpu_mem_usage=True,\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "output.clear()"
      ],
      "metadata": {
        "id": "SFGF5LSbZBil"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a prompt for zero-shot machine translation"
      ],
      "metadata": {
        "id": "MDR1g6YDqZfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero-shot prompt template\n",
        "prompt = \"\"\"{source_language}: {source_text}\n",
        "{target_language}:\"\"\"\n",
        "\n",
        "# A function that will take care of generating the output\n",
        "def generate(input_dict):\n",
        "    text = prompt.format(**input_dict)\n",
        "    input_ids = tokenizer(text, return_tensors='pt').input_ids.cuda()\n",
        "    prediction = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=False,\n",
        "        eos_token_id=tokenizer('\\n').input_ids\n",
        "    )\n",
        "    decoded_prediction = tokenizer.decode(prediction[0, input_ids.size(1):]).strip()\n",
        "    output.clear()\n",
        "\n",
        "    return decoded_prediction"
      ],
      "metadata": {
        "id": "-G6IK4JBayiz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Translate!"
      ],
      "metadata": {
        "id": "8F7hGAMcqqBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_text = \"Can NorMistral still translate, even when quantized in Google Colab?\" # @param {type:\"string\"}\n",
        "source_language = \"Engelsk\" # @param [\"Engelsk\", \"Bokmål\", \"Nynorsk\"]\n",
        "target_language = \"Bokm\\xE5l\" # @param [\"Engelsk\", \"Bokmål\", \"Nynorsk\"]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NsMfsv6gqvD4"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_text = generate({\n",
        "    \"source_text\": source_text,\n",
        "    \"source_language\": source_language,\n",
        "    \"target_language\": target_language\n",
        "})\n",
        "\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "aKFKAU3Da1vS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1214fb30-a487-4590-c4af-e7278c609790"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kan NorMistral fortsatt oversette, selv når kvantisert i Google Colab?\n"
          ]
        }
      ]
    }
  ]
}